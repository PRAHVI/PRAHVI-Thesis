\chapter{Architecture}
\section{Architectural Diagram}
This system has a data-flow architecture (see Figure~\ref{architectualDiagram}), starts from "Headset" and move clock-wise to "Audio". The reason of choosing this architecture is because there are constant inputs received by the system, and the inputs in general goes through the same route within the system. The data-flow architecture also has build in concurrency, which can speeds up the process, especially the platform of the product is embedded.
\begin{figure}
	\label{architectualDiagram}
	\centering
    \includegraphics[scale = 0.2]{ArchitectureDiagram.png}
    
    \caption{Architectural Diagram}
\end{figure}


\section{Backend}
The backend of PRAHVI is a flask web application. It holds all the functionality related to the computer vision, optical character recognition, and text summarization related tasks PRAHVI requires.

The interface of the backend is a simple set of HTTP endpoints which are summarized as follows:


The PRAHVI iOS application makes calls to these endpoints in order to translate an image to text as well as summarize the text using the TFIDF.

Due to the limited compute resources in our mobile device, having the backend end hosted on a server allows for a faster processing time as noted in our testbench.


\section{System Flow}
Once the camera captured the image and send the image to the smart phone, the image will pass through the pre-processing stage to detect whether the image will be processed or not. Once the image passed the pre-processing stage and is also processed for text, it is then output as audio feedback.

\section{Text Extraction}
\subsection{Image Pre-processing}When the smart phone application received the image, the image is then passed through 2 filters. The first filter detects the blurriness of the image. If the image is blurred, the phone will reject the image because it is hard to extract useful information from a blurred imaged. If the image passed the blurriness test, it will then be used to compare to the previous detected image for similarity. If the image is similar (or the same), the image will also be rejected, this is because the information of the same document is stored in the device from the previous capture. These 2 filters help to improve the efficiency of the system and save time waited by the user.

\subsubsection{Blurriness Test}To test for blurriness, variance of Laplacian is used. The image is treated as a 2-dimensional matrix, and the variance of Laplacian of the matrix is generated. The variance of Laplacian of the image is then compared to a threshold value (the threshold value used for this system is 50). If the variance of Laplacian of the image is lower than the threshold, then the image is blurred, otherwise, it is not.\subsubsection{Similarity Test}The similarity test uses the accelerated-KAZE (AKAZE) local features matching\footnote{http://docs.opencv.org/3.0-beta/doc/tutorials/features2d/akaze\_matching/akaze\_matching.html}. The AKAZE local features matching returns a list of matches between the two images. If the number of matches is above the threshold (the threshold value used for this system is 1000), then the images is said to be similar.
\subsection{Image Processing}When the image passed the blurriness test and the similarity test, the system will try to detect the text area in the image and extract the text area. The extracted text area is then send to the Tesseract Optical Character Recognition (OCR) Engine to convert the image to computer encoded characters.\subsubsection{Text Area Extraction}To extract the text area, a copy of the image is first blurred (Gaussian Blur is used in implementation) to detect the edges in the image (Canny Edge Detection is used in implementation). From the edges of the image, contours are then collected and sorted in descending order based on their area.


From the list of sorted contours, the biggest contour that can be approximates to a quadrilateral is identified as the text area. The text area is then applied with a perspective transformation to convert the text area to a rectangle such that as if the document is scanned. This increases the accuracy of the result from Tesseract OCR Engine. \subsubsection{Text Detection}The transformed text area is processed by Tesseract OCR Engine to identify the bounding boxes of the text in the image and convert the image to computer encoded characters. The computer encoded characters is then processed to extract key featured and feedback to the user.

\section{Text Summarization}
In order to provide users with a summary of the text we have extracted we use an algorithm called Term Frequency-Inverse Document Frequency (TFIDF) .

The goals of TFIDF are to obtain statistically important keywords from a text article. 
It does this by giving each word in the target document a score based of two statistics, the word frequency and the inverse document frequency.
The word frequency is simply just the number of times the word appears in the target document and the inverse document frequency is calculated by taking the log of the total number of documents in the corpus divided by the number of documents that the word appears in.

The final score for each word is calculated as follows: tf * idf

The rationale behind the tfidf algorithm is to reduce the importance of words that appear often yet have no overall significance. Examples of such words are: the, and, is, etc.

The reason why tfidf was chosen for our text summary is because it's one of the more popular and well known term-weighting schemes, it's easy to implement, and once it's set up it is computationally fast.

The corpus of documents are gathered by us. Our goal was to accumulate documents in the news domain so that way our text summarization would be inline with the domain of PRAHVI functional requirements. Our strategy was to build our corpus by scraping the top online news repositories for all of there news articles. 

We took the 50 top online news websites according to the following internet article and we used a python library called newspaper to gather the body text every article currently live on the site.

Once our corpus was collected we precomputed the inverse document frequencies for each term in our corpus.